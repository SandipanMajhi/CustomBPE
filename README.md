# CustomBPE
BPE Tokenizer for Masked Language Modelling

### Training ###
The training was done on Bookcorpus text. The tokenizer part makes the model `uncased`. 
`Uniique Vocabulary Size = 100,000`

Check out the `bpe_train.py` for training purposes.

### Tests:
Quite small number of test have been done. Check out at `test.ipynb` file. 

### Development
This tokenizer was built mainly for tineeBERT which is my current project. If you find issues. Please submit in `Issues` tab.
